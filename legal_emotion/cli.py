import argparse
import json
from .compare import compare_scores, explain_pair
from .token_compare import (
    compare_token_clouds,
    explain_token_pair,
)
from .predict import predict
from .prepare import prepare_txt_dir
from .score import score_txt_dir
from .sentiment_config import load_sentiment_config
from .sentiment_predict import predict_sentiment
from .sentiment_prepare import prepare_sigmalaw_absa
from .sentiment_report import (
    evaluate_sentiment_checkpoint,
    write_json as write_sentiment_report_json,
)
from .sentiment_score import score_txt_dir_sentiment
from .sentiment_train import run_sentiment_training
from .gold_eval import (
    evaluate_checkpoint as evaluate_emotion_checkpoint,
    write_json as write_emotion_report_json,
)
from .train import run_training
from .utils import load_config
from .lexicon import tokenize as lex_tokenize


def main():
    parser = argparse.ArgumentParser()
    sub = parser.add_subparsers(dest='cmd')
    t = sub.add_parser('train')
    t.add_argument(
        '--config',
        type=str,
        default=None,
    )
    e = sub.add_parser('eval')
    e.add_argument(
        '--checkpoint',
        type=str,
        required=True,
    )
    e.add_argument(
        '--texts',
        type=str,
        nargs='+',
        required=True,
    )
    e.add_argument(
        '--config',
        type=str,
        default=None,
    )
    p = sub.add_parser('prepare')
    p.add_argument(
        '--input_dir',
        type=str,
        required=True,
    )
    p.add_argument(
        '--train_out',
        type=str,
        default='data/train.jsonl',
    )
    p.add_argument(
        '--dev_out',
        type=str,
        default='data/dev.jsonl',
    )
    p.add_argument(
        '--dev_ratio',
        type=float,
        default=0.1,
    )
    p.add_argument(
        '--recursive',
        action='store_true',
    )
    p.add_argument(
        '--limit',
        type=int,
        default=None,
    )
    p.add_argument(
        '--stride',
        type=int,
        default=0,
    )
    p.add_argument(
        '--config',
        type=str,
        default=None,
    )
    sp = sub.add_parser('sentiment_prepare')
    sp.add_argument(
        '--out_dir',
        type=str,
        default='data/sigmalaw_absa',
    )
    sp.add_argument(
        '--csv_path',
        type=str,
        default=None,
    )
    sp.add_argument(
        '--dev_ratio',
        type=float,
        default=0.1,
    )
    sp.add_argument(
        '--seed',
        type=int,
        default=13,
    )
    sp.add_argument(
        '--no_download',
        action='store_true',
    )
    st = sub.add_parser('sentiment_train')
    st.add_argument(
        '--config',
        type=str,
        default=None,
    )
    se = sub.add_parser('sentiment_eval')
    se.add_argument(
        '--checkpoint',
        type=str,
        required=True,
    )
    se.add_argument(
        '--texts',
        type=str,
        nargs='+',
        required=True,
    )
    se.add_argument(
        '--config',
        type=str,
        default=None,
    )
    ss = sub.add_parser('sentiment_score')
    ss.add_argument(
        '--checkpoint',
        type=str,
        required=True,
    )
    ss.add_argument(
        '--input_dir',
        type=str,
        required=True,
    )
    ss.add_argument(
        '--output',
        type=str,
        required=True,
    )
    ss.add_argument(
        '--recursive',
        action='store_true',
    )
    ss.add_argument(
        '--limit',
        type=int,
        default=None,
    )
    ss.add_argument(
        '--stride',
        type=int,
        default=0,
    )
    ss.add_argument(
        '--batch_size',
        type=int,
        default=None,
    )
    ss.add_argument(
        '--config',
        type=str,
        default=None,
    )
    sr = sub.add_parser('sentiment_report')
    sr.add_argument(
        '--checkpoint',
        type=str,
        required=True,
    )
    sr.add_argument(
        '--data',
        type=str,
        required=True,
    )
    sr.add_argument(
        '--batch_size',
        type=int,
        default=None,
    )
    sr.add_argument(
        '--limit',
        type=int,
        default=None,
    )
    sr.add_argument(
        '--save_json',
        type=str,
        default=None,
    )
    sr.add_argument(
        '--config',
        type=str,
        default=None,
    )
    ge = sub.add_parser('gold_eval')
    ge.add_argument(
        '--checkpoint',
        type=str,
        required=True,
    )
    ge.add_argument(
        '--data',
        type=str,
        required=True,
    )
    ge.add_argument(
        '--batch_size',
        type=int,
        default=None,
    )
    ge.add_argument(
        '--limit',
        type=int,
        default=None,
    )
    ge.add_argument(
        '--save_json',
        type=str,
        default=None,
    )
    ge.add_argument(
        '--config',
        type=str,
        default=None,
    )
    s = sub.add_parser('score')
    s.add_argument(
        '--checkpoint',
        type=str,
        required=True,
    )
    s.add_argument(
        '--input_dir',
        type=str,
        required=True,
    )
    s.add_argument(
        '--output',
        type=str,
        required=True,
    )
    s.add_argument(
        '--recursive',
        action='store_true',
    )
    s.add_argument(
        '--limit',
        type=int,
        default=None,
    )
    s.add_argument(
        '--stride',
        type=int,
        default=0,
    )
    s.add_argument(
        '--batch_size',
        type=int,
        default=None,
    )
    s.add_argument(
        '--config',
        type=str,
        default=None,
    )
    gs = sub.add_parser('goemotions_score')
    gs.add_argument(
        '--input_dir',
        type=str,
        required=True,
    )
    gs.add_argument(
        '--output',
        type=str,
        required=True,
    )
    gs.add_argument(
        '--model',
        type=str,
        default=None,
    )
    gs.add_argument(
        '--recursive',
        action='store_true',
    )
    gs.add_argument(
        '--limit',
        type=int,
        default=None,
    )
    gs.add_argument(
        '--stride',
        type=int,
        default=0,
    )
    gs.add_argument(
        '--batch_size',
        type=int,
        default=None,
    )
    gs.add_argument(
        '--max_length',
        type=int,
        default=None,
    )
    gs.add_argument(
        '--scale_per_1k_words',
        type=float,
        default=20.0,
    )
    gs.add_argument(
        '--config',
        type=str,
        default=None,
    )
    c = sub.add_parser('compare')
    c.add_argument(
        '--input',
        type=str,
        required=True,
    )
    c.add_argument(
        '--output',
        type=str,
        required=True,
    )
    c.add_argument(
        '--format',
        type=str,
        default='neighbours',
        choices=('neighbours', 'matrix'),
    )
    c.add_argument(
        '--topk',
        type=int,
        default=10,
    )
    c.add_argument(
        '--measure',
        type=str,
        default=None,
    )
    c.add_argument(
        '--mode',
        type=str,
        default=None,
    )
    c.add_argument(
        '--cost',
        type=str,
        default=None,
    )
    c.add_argument(
        '--epsilon',
        type=float,
        default=None,
    )
    c.add_argument(
        '--iters',
        type=int,
        default=None,
    )
    c.add_argument(
        '--reg_m',
        type=float,
        default=None,
    )
    c.add_argument(
        '--style_measure',
        type=str,
        default=None,
    )
    c.add_argument(
        '--style_mode',
        type=str,
        default=None,
    )
    c.add_argument(
        '--no_style',
        action='store_true',
    )
    c.add_argument(
        '--no_explain',
        action='store_true',
    )
    c.add_argument(
        '--top_flows',
        type=int,
        default=8,
    )
    c.add_argument(
        '-vis',
        '--vis',
        action='store_true',
    )
    c.add_argument(
        '--limit',
        type=int,
        default=None,
    )
    c.add_argument(
        '--config',
        type=str,
        default=None,
    )
    x = sub.add_parser('explain')
    x.add_argument(
        '--input',
        type=str,
        required=True,
    )
    x.add_argument(
        '--i',
        type=int,
        required=True,
    )
    x.add_argument(
        '--j',
        type=int,
        required=True,
    )
    x.add_argument(
        '--output',
        type=str,
        default=None,
    )
    x.add_argument(
        '--measure',
        type=str,
        default=None,
    )
    x.add_argument(
        '--mode',
        type=str,
        default=None,
    )
    x.add_argument(
        '--style_measure',
        type=str,
        default=None,
    )
    x.add_argument(
        '--style_mode',
        type=str,
        default=None,
    )
    x.add_argument(
        '--cost',
        type=str,
        default=None,
    )
    x.add_argument(
        '--epsilon',
        type=float,
        default=None,
    )
    x.add_argument(
        '--iters',
        type=int,
        default=None,
    )
    x.add_argument(
        '--reg_m',
        type=float,
        default=None,
    )
    x.add_argument(
        '--top_flows',
        type=int,
        default=8,
    )
    x.add_argument(
        '--config',
        type=str,
        default=None,
    )
    tc = sub.add_parser('compare_tokens')
    tc.add_argument(
        '--input',
        type=str,
        required=True,
    )
    tc.add_argument(
        '--output',
        type=str,
        required=True,
    )
    tc.add_argument(
        '--format',
        type=str,
        default='neighbours',
        choices=('neighbours', 'matrix'),
    )
    tc.add_argument(
        '--topk',
        type=int,
        default=10,
    )
    tc.add_argument(
        '--candidate_k',
        type=int,
        default=0,
    )
    tc.add_argument(
        '--mode',
        type=str,
        default=None,
    )
    tc.add_argument(
        '--focus',
        type=str,
        default='emotional',
        choices=('all', 'emotional'),
    )
    tc.add_argument(
        '--cost',
        type=str,
        default='embedding_vad',
        choices=('embedding', 'vad', 'embedding_vad'),
    )
    tc.add_argument(
        '--embed_model',
        type=str,
        default=None,
    )
    tc.add_argument(
        '--embed_backend',
        type=str,
        default=None,
        choices=('encoder', 'input_embeddings'),
    )
    tc.add_argument(
        '--embed_pooling',
        type=str,
        default=None,
        choices=('cls', 'mean'),
    )
    tc.add_argument(
        '--embed_batch_size',
        type=int,
        default=None,
    )
    tc.add_argument(
        '--embed_max_length',
        type=int,
        default=None,
    )
    tc.add_argument(
        '--embed_prompt_mode',
        type=str,
        default=None,
    )
    tc.add_argument(
        '--embed_prompt_text',
        type=str,
        default=None,
    )
    tc.add_argument(
        '--alpha_embed',
        type=float,
        default=0.5,
    )
    tc.add_argument(
        '--beta_vad',
        type=float,
        default=0.5,
    )
    tc.add_argument(
        '--vad_threshold',
        type=float,
        default=0.45,
    )
    tc.add_argument(
        '--emotional_vocab',
        type=str,
        default='auto',
        choices=(
            'auto',
            'lexicon',
            'vad',
            'lexicon_or_vad',
        ),
    )
    tc.add_argument(
        '--vad_min_arousal_vad_only',
        type=float,
        default=0.45,
    )
    tc.add_argument(
        '--max_ngram',
        type=int,
        default=0,
    )
    tc.add_argument(
        '--epsilon',
        type=float,
        default=None,
    )
    tc.add_argument(
        '--iters',
        type=int,
        default=None,
    )
    tc.add_argument(
        '--reg_m',
        type=float,
        default=None,
    )
    tc.add_argument(
        '--weight',
        type=str,
        default='tfidf',
    )
    tc.add_argument(
        '--term_weights_path',
        type=str,
        default=None,
    )
    tc.add_argument(
        '--term_weight_power',
        type=float,
        default=None,
    )
    tc.add_argument(
        '--term_weight_min',
        type=float,
        default=None,
    )
    tc.add_argument(
        '--term_weight_max',
        type=float,
        default=None,
    )
    tc.add_argument(
        '--term_weight_mix',
        type=float,
        default=None,
    )
    tc.add_argument(
        '--term_weight_default',
        type=float,
        default=None,
    )
    tc.add_argument(
        '--vad_imputed_path',
        type=str,
        default=None,
    )
    tc.add_argument(
        '--max_terms',
        type=int,
        default=256,
    )
    tc.add_argument(
        '--min_token_len',
        type=int,
        default=2,
    )
    tc.add_argument(
        '--no_stopwords',
        action='store_true',
    )
    tc.add_argument(
        '--stopwords_file',
        type=str,
        default=None,
    )
    tc.add_argument(
        '--negation_window',
        type=int,
        default=None,
    )
    tc.add_argument(
        '--negators',
        nargs='*',
        default=None,
    )
    tc.add_argument(
        '--drop_top_df',
        type=int,
        default=100,
    )
    tc.add_argument(
        '--no_explain',
        action='store_true',
    )
    tc.add_argument(
        '--top_flows',
        type=int,
        default=8,
    )
    tc.add_argument(
        '-vis',
        '--vis',
        action='store_true',
    )
    tc.add_argument(
        '--limit',
        type=int,
        default=None,
    )
    tc.add_argument(
        '--config',
        type=str,
        default=None,
    )
    xt = sub.add_parser('explain_tokens')
    xt.add_argument(
        '--input',
        type=str,
        required=True,
    )
    xt.add_argument(
        '--i',
        type=int,
        required=True,
    )
    xt.add_argument(
        '--j',
        type=int,
        required=True,
    )
    xt.add_argument(
        '--mode',
        type=str,
        default=None,
    )
    xt.add_argument(
        '--focus',
        type=str,
        default='emotional',
        choices=('all', 'emotional'),
    )
    xt.add_argument(
        '--cost',
        type=str,
        default='embedding_vad',
        choices=('embedding', 'vad', 'embedding_vad'),
    )
    xt.add_argument(
        '--embed_model',
        type=str,
        default=None,
    )
    xt.add_argument(
        '--embed_backend',
        type=str,
        default=None,
        choices=('encoder', 'input_embeddings'),
    )
    xt.add_argument(
        '--embed_pooling',
        type=str,
        default=None,
        choices=('cls', 'mean'),
    )
    xt.add_argument(
        '--embed_batch_size',
        type=int,
        default=None,
    )
    xt.add_argument(
        '--embed_max_length',
        type=int,
        default=None,
    )
    xt.add_argument(
        '--embed_prompt_mode',
        type=str,
        default=None,
    )
    xt.add_argument(
        '--embed_prompt_text',
        type=str,
        default=None,
    )
    xt.add_argument(
        '--alpha_embed',
        type=float,
        default=0.5,
    )
    xt.add_argument(
        '--beta_vad',
        type=float,
        default=0.5,
    )
    xt.add_argument(
        '--vad_threshold',
        type=float,
        default=0.45,
    )
    xt.add_argument(
        '--emotional_vocab',
        type=str,
        default='auto',
        choices=(
            'auto',
            'lexicon',
            'vad',
            'lexicon_or_vad',
        ),
    )
    xt.add_argument(
        '--vad_min_arousal_vad_only',
        type=float,
        default=0.45,
    )
    xt.add_argument(
        '--max_ngram',
        type=int,
        default=0,
    )
    xt.add_argument(
        '--epsilon',
        type=float,
        default=None,
    )
    xt.add_argument(
        '--iters',
        type=int,
        default=None,
    )
    xt.add_argument(
        '--reg_m',
        type=float,
        default=None,
    )
    xt.add_argument(
        '--weight',
        type=str,
        default='tfidf',
    )
    xt.add_argument(
        '--term_weights_path',
        type=str,
        default=None,
    )
    xt.add_argument(
        '--term_weight_power',
        type=float,
        default=None,
    )
    xt.add_argument(
        '--term_weight_min',
        type=float,
        default=None,
    )
    xt.add_argument(
        '--term_weight_max',
        type=float,
        default=None,
    )
    xt.add_argument(
        '--term_weight_mix',
        type=float,
        default=None,
    )
    xt.add_argument(
        '--term_weight_default',
        type=float,
        default=None,
    )
    xt.add_argument(
        '--vad_imputed_path',
        type=str,
        default=None,
    )
    xt.add_argument(
        '--max_terms',
        type=int,
        default=256,
    )
    xt.add_argument(
        '--min_token_len',
        type=int,
        default=2,
    )
    xt.add_argument(
        '--no_stopwords',
        action='store_true',
    )
    xt.add_argument(
        '--stopwords_file',
        type=str,
        default=None,
    )
    xt.add_argument(
        '--negation_window',
        type=int,
        default=None,
    )
    xt.add_argument(
        '--negators',
        nargs='*',
        default=None,
    )
    xt.add_argument(
        '--drop_top_df',
        type=int,
        default=100,
    )
    xt.add_argument(
        '--top_flows',
        type=int,
        default=8,
    )
    xt.add_argument(
        '--config',
        type=str,
        default=None,
    )
    stt = sub.add_parser('silver_teacher')
    stt.add_argument(
        '--input',
        required=True,
    )
    stt.add_argument(
        '--output',
        required=True,
    )
    stt.add_argument(
        '--config',
        default=None,
    )
    stt.add_argument(
        '--model',
        default=None,
    )
    stt.add_argument(
        '--batch_size',
        type=int,
        default=None,
    )
    stt.add_argument(
        '--max_length',
        type=int,
        default=None,
    )
    stt.add_argument(
        '--no_truncate',
        action='store_true',
    )
    opts = parser.parse_args()
    if opts.cmd == 'train':
        setup = load_config(opts.config)
        run_training(setup)
    elif opts.cmd == 'eval':
        setup = load_config(opts.config)
        (
            probs,
            vad,
            counts,
        ) = predict(
            opts.texts,
            opts.checkpoint,
            setup,
        )
        rows = []
        for i, text in enumerate(opts.texts):
            guess_mass = float(counts[i].sum().item())
            n_words = len(lex_tokenize(text))
            rows.append({
                    'text': text,
                    'probs': probs[i].tolist(),
                    'vad': vad[i].tolist(),
                    'pred_counts': counts[i].tolist(),
                    'pred_mass': guess_mass,
                    'pred_per_1k_words': float(guess_mass
                        / max(
                            n_words,
                            1,
                        )
                        * 1000.0),
                    'emotions': setup.emotions,
                })
        print(json.dumps(
                rows,
                indent=2,
            ))
    elif opts.cmd == 'prepare':
        setup = load_config(opts.config)
        stats = prepare_txt_dir(
            opts.input_dir,
            opts.train_out,
            opts.dev_out,
            tokenizer_name=setup.model_name,
            max_length=setup.max_length,
            dev_ratio=opts.dev_ratio,
            seed=setup.seed,
            recursive=opts.recursive,
            limit=opts.limit,
            stride=opts.stride,
        )
        print(json.dumps(
                stats,
                indent=2,
            ))
    elif opts.cmd == 'sentiment_prepare':
        stats = prepare_sigmalaw_absa(
            out_dir=opts.out_dir,
            csv_path=opts.csv_path,
            dev_ratio=opts.dev_ratio,
            seed=opts.seed,
            download=not bool(getattr(
                    opts,
                    'no_download',
                    False,
                )),
        )
        print(json.dumps(
                stats,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'sentiment_train':
        setup = load_sentiment_config(opts.config)
        run_sentiment_training(setup)
    elif opts.cmd == 'sentiment_eval':
        setup = load_sentiment_config(opts.config)
        (
            probs,
            guess_sent,
        ) = predict_sentiment(
            opts.texts,
            opts.checkpoint,
            setup,
        )
        rows = []
        for i, text in enumerate(opts.texts):
            rows.append({
                    'text': text,
                    'probs': probs[i].tolist(),
                    'pred_sentiment': int(guess_sent[i].item()),
                    'sentiment_score': float(probs[i, 2].item()
                        - probs[i, 0].item()),
                })
        print(json.dumps(
                rows,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'sentiment_score':
        stats = score_txt_dir_sentiment(
            checkpoint=opts.checkpoint,
            input_dir=opts.input_dir,
            output_jsonl=opts.output,
            cfg_path=opts.config,
            recursive=opts.recursive,
            limit=opts.limit,
            stride=opts.stride,
            batch_size=opts.batch_size,
        )
        print(json.dumps(
                stats,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'sentiment_report':
        metrics = evaluate_sentiment_checkpoint(
            checkpoint=opts.checkpoint,
            data_path=opts.data,
            cfg_path=opts.config,
            batch_size=opts.batch_size,
            limit=opts.limit,
        )
        if opts.save_json:
            write_sentiment_report_json(
                opts.save_json,
                metrics,
            )
        print(json.dumps(
                metrics,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'gold_eval':
        metrics = evaluate_emotion_checkpoint(
            checkpoint=opts.checkpoint,
            data_path=opts.data,
            cfg_path=opts.config,
            batch_size=opts.batch_size,
            limit=opts.limit,
        )
        if opts.save_json:
            write_emotion_report_json(
                opts.save_json,
                metrics,
            )
        print(json.dumps(
                metrics,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'score':
        stats = score_txt_dir(
            checkpoint=opts.checkpoint,
            input_dir=opts.input_dir,
            output_jsonl=opts.output,
            cfg_path=opts.config,
            recursive=opts.recursive,
            limit=opts.limit,
            stride=opts.stride,
            batch_size=opts.batch_size,
        )
        print(json.dumps(
                stats,
                indent=2,
            ))
    elif opts.cmd == 'goemotions_score':
        from .goemotions_score import (
            score_txt_dir_goemotions,
        )

        stats = score_txt_dir_goemotions(
            input_dir=opts.input_dir,
            output_jsonl=opts.output,
            cfg_path=opts.config,
            model_name=opts.model,
            recursive=opts.recursive,
            limit=opts.limit,
            stride=opts.stride,
            batch_size=opts.batch_size,
            max_length=opts.max_length,
            intensity_scale_per_1k_words=float(opts.scale_per_1k_words),
        )
        print(json.dumps(
                stats,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'silver_teacher':
        from .silver_teacher import make_teacher_silver

        stats = make_teacher_silver(
            input_path=opts.input,
            output_path=opts.output,
            cfg_path=opts.config,
            model_name=opts.model,
            batch_size=opts.batch_size,
            max_length=opts.max_length,
            truncate_to_max_length=not bool(opts.no_truncate),
        )
        print(json.dumps(
                stats,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'compare':
        stats = compare_scores(
            input_jsonl=opts.input,
            output_path=opts.output,
            cfg_path=opts.config,
            mode=opts.mode,
            cost=opts.cost,
            measure=opts.measure,
            style_mode=opts.style_mode,
            style_measure=opts.style_measure,
            include_style=not bool(getattr(
                    opts,
                    'no_style',
                    False,
                )),
            include_explain=not bool(getattr(
                    opts,
                    'no_explain',
                    False,
                )),
            epsilon=opts.epsilon,
            iters=opts.iters,
            reg_m=opts.reg_m,
            fmt=opts.format,
            topk=opts.topk,
            top_flows=opts.top_flows,
            limit=opts.limit,
            vis=bool(getattr(
                    opts,
                    'vis',
                    False,
                )),
        )
        print(json.dumps(
                stats,
                indent=2,
            ))
    elif opts.cmd == 'explain':
        payload = explain_pair(
            input_jsonl=opts.input,
            i=opts.i,
            j=opts.j,
            output_path=opts.output,
            cfg_path=opts.config,
            cost=opts.cost,
            mode=opts.mode,
            measure=opts.measure,
            style_mode=opts.style_mode,
            style_measure=opts.style_measure,
            epsilon=opts.epsilon,
            iters=opts.iters,
            reg_m=opts.reg_m,
            top_flows=opts.top_flows,
        )
        print(json.dumps(
                payload,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'compare_tokens':
        stats = compare_token_clouds(
            input_jsonl=opts.input,
            output_path=opts.output,
            cfg_path=opts.config,
            fmt=opts.format,
            topk=opts.topk,
            candidate_k=opts.candidate_k,
            mode=opts.mode,
            focus=opts.focus,
            cost=opts.cost,
            embed_model=opts.embed_model,
            embed_backend=opts.embed_backend,
            embed_pooling=opts.embed_pooling,
            embed_batch_size=opts.embed_batch_size,
            embed_max_length=opts.embed_max_length,
            embed_prompt_mode=opts.embed_prompt_mode,
            embed_prompt_text=opts.embed_prompt_text,
            alpha_embed=opts.alpha_embed,
            beta_vad=opts.beta_vad,
            vad_threshold=opts.vad_threshold,
            emotional_vocab=opts.emotional_vocab,
            vad_min_arousal_vad_only=opts.vad_min_arousal_vad_only,
            max_ngram=opts.max_ngram,
            epsilon=opts.epsilon,
            iters=opts.iters,
            reg_m=opts.reg_m,
            weight=opts.weight,
            term_weights_path=opts.term_weights_path,
            term_weight_power=opts.term_weight_power,
            term_weight_min=opts.term_weight_min,
            term_weight_max=opts.term_weight_max,
            term_weight_mix=opts.term_weight_mix,
            term_weight_default=opts.term_weight_default,
            vad_imputed_path=opts.vad_imputed_path,
            max_terms=opts.max_terms,
            min_token_len=opts.min_token_len,
            stopwords=not bool(getattr(
                    opts,
                    'no_stopwords',
                    False,
                )),
            stopwords_file=opts.stopwords_file,
            negation_window=opts.negation_window,
            negators=opts.negators,
            drop_top_df=opts.drop_top_df,
            include_explain=not bool(getattr(
                    opts,
                    'no_explain',
                    False,
                )),
            top_flows=opts.top_flows,
            limit=opts.limit,
            vis=bool(getattr(
                    opts,
                    'vis',
                    False,
                )),
        )
        print(json.dumps(
                stats,
                indent=2,
                ensure_ascii=False,
            ))
    elif opts.cmd == 'explain_tokens':
        payload = explain_token_pair(
            input_jsonl=opts.input,
            i=opts.i,
            j=opts.j,
            cfg_path=opts.config,
            mode=opts.mode,
            focus=opts.focus,
            cost=opts.cost,
            embed_model=opts.embed_model,
            embed_backend=opts.embed_backend,
            embed_pooling=opts.embed_pooling,
            embed_batch_size=opts.embed_batch_size,
            embed_max_length=opts.embed_max_length,
            embed_prompt_mode=opts.embed_prompt_mode,
            embed_prompt_text=opts.embed_prompt_text,
            alpha_embed=opts.alpha_embed,
            beta_vad=opts.beta_vad,
            vad_threshold=opts.vad_threshold,
            emotional_vocab=opts.emotional_vocab,
            vad_min_arousal_vad_only=opts.vad_min_arousal_vad_only,
            max_ngram=opts.max_ngram,
            epsilon=opts.epsilon,
            iters=opts.iters,
            reg_m=opts.reg_m,
            weight=opts.weight,
            term_weights_path=opts.term_weights_path,
            term_weight_power=opts.term_weight_power,
            term_weight_min=opts.term_weight_min,
            term_weight_max=opts.term_weight_max,
            term_weight_mix=opts.term_weight_mix,
            term_weight_default=opts.term_weight_default,
            vad_imputed_path=opts.vad_imputed_path,
            max_terms=opts.max_terms,
            min_token_len=opts.min_token_len,
            stopwords=not bool(getattr(
                    opts,
                    'no_stopwords',
                    False,
                )),
            stopwords_file=opts.stopwords_file,
            negation_window=opts.negation_window,
            negators=opts.negators,
            drop_top_df=opts.drop_top_df,
            top_flows=opts.top_flows,
        )
        print(json.dumps(
                payload,
                indent=2,
                ensure_ascii=False,
            ))
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
